{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model with parameters: num_topics=5, chunksize=2000, passes=10, iterations=200\n",
      "An error occurred: [Errno 2] No such file or directory: 'input'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 148\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mfor\u001b[39;00m num_topics, chunksize, passes, iterations \u001b[39min\u001b[39;00m param_combinations:\n\u001b[1;32m    147\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mRunning model with parameters: num_topics=\u001b[39m\u001b[39m{\u001b[39;00mnum_topics\u001b[39m}\u001b[39;00m\u001b[39m, chunksize=\u001b[39m\u001b[39m{\u001b[39;00mchunksize\u001b[39m}\u001b[39;00m\u001b[39m, passes=\u001b[39m\u001b[39m{\u001b[39;00mpasses\u001b[39m}\u001b[39;00m\u001b[39m, iterations=\u001b[39m\u001b[39m{\u001b[39;00miterations\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 148\u001b[0m     modeler\u001b[39m.\u001b[39;49mrun_with_params(num_topics, chunksize, passes, iterations)\n",
      "Cell \u001b[0;32mIn[11], line 119\u001b[0m, in \u001b[0;36mTopicModeler.run_with_params\u001b[0;34m(self, num_topics, chunksize, passes, iterations)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_params\u001b[39m(\u001b[39mself\u001b[39m, num_topics, chunksize, passes, iterations):\n\u001b[1;32m    118\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_documents()\n\u001b[1;32m    120\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_documents()\n\u001b[1;32m    121\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_bigrams()\n",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m, in \u001b[0;36mTopicModeler.load_documents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_documents\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_path):\n\u001b[1;32m     28\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_path, filename), \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     29\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocs\u001b[39m.\u001b[39mappend(f\u001b[39m.\u001b[39mread())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases, LdaModel\n",
    "from gensim.models import phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "class TopicModeler:\n",
    "    def __init__(self, input_path):\n",
    "        self.input_path = input_path\n",
    "        self.docs = []\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        self.model = None\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    def load_documents(self):\n",
    "        for filename in os.listdir(self.input_path):\n",
    "            with open(os.path.join(self.input_path, filename), 'r', encoding='utf-8') as f:\n",
    "                self.docs.append(f.read())\n",
    "        print(f\"Loaded {len(self.docs)} documents\")\n",
    "        if self.docs:\n",
    "            print(f\"Sample document: {self.docs[0][:100]}...\")  \n",
    "\n",
    "    def preprocess_documents(self):\n",
    "        self.docs = [[token.lower() for token in self.tokenizer.tokenize(doc)] for doc in self.docs]\n",
    "        \n",
    "        self.docs = [[token for token in doc if not token.isnumeric() and len(token) > 1] for doc in self.docs]\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        self.docs = [[word for word in doc if word not in stop_words] for doc in self.docs]\n",
    "        \n",
    "        self.docs = [[self.lemmatizer.lemmatize(token) for token in doc] for doc in self.docs]\n",
    "\n",
    "        print(f\"After preprocessing: {len(self.docs)} documents\")\n",
    "        if self.docs:\n",
    "            print(f\"Sample preprocessed document: {self.docs[0][:10]}...\") \n",
    "\n",
    "    def add_bigrams(self):\n",
    "        bigram = Phrases(self.docs, min_count=5, threshold=100)\n",
    "        for idx in range(len(self.docs)):\n",
    "            for token in bigram[self.docs[idx]]:\n",
    "                if '_' in token:\n",
    "                    self.docs[idx].append(token)\n",
    "        print(f\"Added bigrams. Sample document: {self.docs[0][:15]}...\")\n",
    "\n",
    "    def create_dictionary_and_corpus(self):\n",
    "        self.dictionary = Dictionary(self.docs)\n",
    "        original_size = len(self.dictionary)\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=0.9)\n",
    "        self.corpus = [self.dictionary.doc2bow(doc) for doc in self.docs]\n",
    "        print(f\"Dictionary size: {len(self.dictionary)} (reduced from {original_size})\")\n",
    "        print(f\"Corpus size: {len(self.corpus)}\")\n",
    "        if self.corpus:\n",
    "            print(f\"Sample corpus entry: {self.corpus[0][:10]}\") \n",
    "\n",
    "    def train_lda_model(self, num_topics=6, chunksize=2000, passes=20, iterations=400):\n",
    "        if not self.corpus or not self.dictionary:\n",
    "            raise ValueError(\"Corpus or dictionary is empty. Check your preprocessing steps.\")\n",
    "        self.model = LdaModel(\n",
    "            corpus=self.corpus,\n",
    "            id2word=self.dictionary,\n",
    "            chunksize=chunksize,\n",
    "            alpha='auto',\n",
    "            eta='auto',\n",
    "            iterations=iterations,\n",
    "            num_topics=num_topics,\n",
    "            passes=passes,\n",
    "            eval_every=None,\n",
    "            random_state=42\n",
    "        )\n",
    "        print(\"LDA model trained successfully\")\n",
    "\n",
    "    def print_model_info(self):\n",
    "        print('Number of unique tokens: %d' % len(self.dictionary))\n",
    "        print('Number of documents: %d' % len(self.corpus))\n",
    "        \n",
    "        top_topics = self.model.top_topics(self.corpus)\n",
    "        avg_topic_coherence = sum([t[1] for t in top_topics]) / self.model.num_topics\n",
    "        print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "        \n",
    "        print(\"\\nTop topics:\")\n",
    "        pprint(self.model.print_topics())\n",
    "        \n",
    "        print('\\nPerplexity: ', self.model.log_perplexity(self.corpus))\n",
    "        \n",
    "        coherence_model_lda = CoherenceModel(model=self.model, texts=self.docs, dictionary=self.dictionary, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "    def visualize_topics(self):\n",
    "        vis = pyLDAvis.gensim.prepare(self.model, self.corpus, self.dictionary)\n",
    "        return vis\n",
    "\n",
    "    def analyze_documents(self):\n",
    "        for i, doc in enumerate(self.docs[:5]): \n",
    "            bow = self.dictionary.doc2bow(doc)\n",
    "            doc_topics = self.model.get_document_topics(bow)\n",
    "            print(f\"\\nDocument {i} topics:\")\n",
    "            pprint(doc_topics)\n",
    "            best_topic = max(doc_topics, key=lambda x: x[1])\n",
    "            print(f\"Best topic: {best_topic[0]}\")\n",
    "            print(f\"Top words in this topic:\")\n",
    "            pprint(self.model.show_topic(best_topic[0]))\n",
    "            print(f\"Original document: {' '.join(doc[:30])}...\")  # Print first 30 words\n",
    "            print()\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            self.load_documents()\n",
    "            self.preprocess_documents()\n",
    "            self.add_bigrams()\n",
    "            self.create_dictionary_and_corpus()\n",
    "            self.train_lda_model()\n",
    "            self.print_model_info()\n",
    "            vis = self.visualize_topics()\n",
    "            self.analyze_documents()\n",
    "            return vis\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    modeler = TopicModeler('txt')  \n",
    "    vis = modeler.run()\n",
    "    \n",
    "    # Save visualization\n",
    "    pyLDAvis.save_html(vis, 'lda_visualization.html')\n",
    "    print(\"Visualization saved as 'lda_visualization.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 13:17:07,341 : INFO : collecting all words and their counts\n",
      "2024-08-02 13:17:07,341 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model with parameters: num_topics=5, chunksize=2000, passes=10, iterations=200\n",
      "Loaded 440 documents\n",
      "Sample document: Pricing for interconnection related to the provision of number portability, as referred to in Articl...\n",
      "After preprocessing: 440 documents\n",
      "Sample preprocessed document: ['pricing', 'interconnection', 'related', 'provision', 'number', 'portability', 'referred', 'article', 'directive', 'ec']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 13:17:07,366 : INFO : collected 23445 token types (unigram + bigrams) from a corpus of 44718 words and 440 sentences\n",
      "2024-08-02 13:17:07,367 : INFO : merged Phrases<23445 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "2024-08-02 13:17:07,368 : INFO : Phrases lifecycle event {'msg': 'built Phrases<23445 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 0.03s', 'datetime': '2024-08-02T13:17:07.368002', 'gensim': '4.3.2', 'python': '3.11.9 (main, Jul 16 2024, 11:07:54) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'created'}\n",
      "2024-08-02 13:17:07,407 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-08-02 13:17:07,428 : INFO : built Dictionary<3666 unique tokens: ['abstract', 'account', 'adoption', 'advance', 'appeal']...> from 440 documents (total 46957 corpus positions)\n",
      "2024-08-02 13:17:07,429 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<3666 unique tokens: ['abstract', 'account', 'adoption', 'advance', 'appeal']...> from 440 documents (total 46957 corpus positions)\", 'datetime': '2024-08-02T13:17:07.429255', 'gensim': '4.3.2', 'python': '3.11.9 (main, Jul 16 2024, 11:07:54) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'created'}\n",
      "2024-08-02 13:17:07,430 : INFO : discarding 1631 tokens: [('abstract', 1), ('confidential', 1), ('confidentiality', 1), ('dissuaded', 1), ('donor', 1), ('ported', 1), ('porting', 1), ('reaching', 1), ('becomes', 1), ('referral', 1)]...\n",
      "2024-08-02 13:17:07,430 : INFO : keeping 2035 tokens which were in no less than 2 and no more than 396 (=90.0%) documents\n",
      "2024-08-02 13:17:07,432 : INFO : resulting dictionary: Dictionary<2035 unique tokens: ['account', 'adoption', 'advance', 'appeal', 'article']...>\n",
      "2024-08-02 13:17:07,448 : INFO : using autotuned alpha, starting with [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "2024-08-02 13:17:07,450 : INFO : using serial LDA version on this node\n",
      "2024-08-02 13:17:07,451 : INFO : running online (multi-pass) LDA training, 5 topics, 10 passes over the supplied corpus of 440 documents, updating model once every 440 documents, evaluating perplexity every 0 documents, iterating 200x with a convergence threshold of 0.001000\n",
      "2024-08-02 13:17:07,451 : INFO : PROGRESS: pass 0, at document #440/440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added bigrams. Sample document: ['pricing', 'interconnection', 'related', 'provision', 'number', 'portability', 'referred', 'article', 'directive', 'ec', 'european', 'parliament', 'council', 'march', 'universal']...\n",
      "Dictionary size: 2035 (reduced from 3666)\n",
      "Corpus size: 440\n",
      "Sample corpus entry: [(0, 1), (1, 1), (2, 1), (3, 3), (4, 3), (5, 2), (6, 2), (7, 2), (8, 1), (9, 1)]\n",
      "Training LDA model with num_topics=5, chunksize=2000, passes=10, iterations=200, alpha=auto, eta=auto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 13:17:07,809 : INFO : optimized alpha [0.13063243, 0.11703815, 0.06443559, 0.13090336, 0.02856037]\n",
      "2024-08-02 13:17:07,810 : INFO : topic #0 (0.131): 0.028*\"article\" + 0.028*\"directive\" + 0.019*\"ec\" + 0.019*\"council\" + 0.018*\"european\" + 0.017*\"regulation\" + 0.014*\"consumer\" + 0.014*\"must\" + 0.013*\"parliament\" + 0.013*\"service\"\n",
      "2024-08-02 13:17:07,810 : INFO : topic #1 (0.117): 0.034*\"directive\" + 0.022*\"article\" + 0.017*\"must\" + 0.016*\"national\" + 0.013*\"interpreted\" + 0.013*\"consumer\" + 0.013*\"service\" + 0.012*\"council\" + 0.011*\"court\" + 0.011*\"law\"\n",
      "2024-08-02 13:17:07,811 : INFO : topic #2 (0.064): 0.032*\"directive\" + 0.031*\"consumer\" + 0.020*\"council\" + 0.018*\"contract\" + 0.017*\"article\" + 0.014*\"term\" + 0.012*\"must\" + 0.011*\"european\" + 0.010*\"ec\" + 0.010*\"interpreted\"\n",
      "2024-08-02 13:17:07,811 : INFO : topic #3 (0.131): 0.027*\"directive\" + 0.021*\"article\" + 0.021*\"consumer\" + 0.021*\"council\" + 0.017*\"ec\" + 0.016*\"term\" + 0.015*\"court\" + 0.015*\"must\" + 0.014*\"unfair\" + 0.013*\"interpreted\"\n",
      "2024-08-02 13:17:07,811 : INFO : topic #4 (0.029): 0.024*\"article\" + 0.022*\"directive\" + 0.018*\"mark\" + 0.014*\"must\" + 0.014*\"regulation\" + 0.012*\"council\" + 0.012*\"trade\" + 0.011*\"trade_mark\" + 0.011*\"eec\" + 0.010*\"interpreted\"\n",
      "2024-08-02 13:17:07,812 : INFO : topic diff=2.393217, rho=1.000000\n",
      "2024-08-02 13:17:07,813 : INFO : PROGRESS: pass 1, at document #440/440\n",
      "2024-08-02 13:17:07,949 : INFO : optimized alpha [0.10181457, 0.09316953, 0.059293702, 0.10319163, 0.027783426]\n",
      "2024-08-02 13:17:07,950 : INFO : topic #0 (0.102): 0.029*\"directive\" + 0.028*\"article\" + 0.021*\"ec\" + 0.021*\"council\" + 0.020*\"european\" + 0.019*\"regulation\" + 0.015*\"parliament\" + 0.014*\"must\" + 0.013*\"meaning\" + 0.013*\"service\"\n",
      "2024-08-02 13:17:07,951 : INFO : topic #1 (0.093): 0.034*\"directive\" + 0.023*\"article\" + 0.017*\"must\" + 0.016*\"national\" + 0.014*\"service\" + 0.013*\"interpreted\" + 0.012*\"consumer\" + 0.012*\"council\" + 0.011*\"law\" + 0.011*\"state\"\n",
      "2024-08-02 13:17:07,951 : INFO : topic #2 (0.059): 0.033*\"directive\" + 0.033*\"consumer\" + 0.019*\"contract\" + 0.019*\"council\" + 0.017*\"article\" + 0.015*\"term\" + 0.013*\"must\" + 0.010*\"interpreted\" + 0.010*\"european\" + 0.010*\"ec\"\n",
      "2024-08-02 13:17:07,951 : INFO : topic #3 (0.103): 0.026*\"directive\" + 0.023*\"consumer\" + 0.021*\"article\" + 0.020*\"council\" + 0.019*\"term\" + 0.016*\"court\" + 0.016*\"must\" + 0.016*\"unfair\" + 0.015*\"ec\" + 0.014*\"national\"\n",
      "2024-08-02 13:17:07,952 : INFO : topic #4 (0.028): 0.025*\"article\" + 0.022*\"mark\" + 0.019*\"directive\" + 0.015*\"trade\" + 0.014*\"regulation\" + 0.014*\"trade_mark\" + 0.012*\"council\" + 0.011*\"must\" + 0.011*\"eec\" + 0.009*\"state\"\n",
      "2024-08-02 13:17:07,952 : INFO : topic diff=0.316557, rho=0.577350\n",
      "2024-08-02 13:17:07,953 : INFO : PROGRESS: pass 2, at document #440/440\n",
      "2024-08-02 13:17:08,053 : INFO : optimized alpha [0.08866139, 0.08013287, 0.05588847, 0.08969429, 0.027495012]\n",
      "2024-08-02 13:17:08,055 : INFO : topic #0 (0.089): 0.029*\"directive\" + 0.028*\"article\" + 0.023*\"ec\" + 0.022*\"council\" + 0.021*\"european\" + 0.020*\"regulation\" + 0.017*\"parliament\" + 0.014*\"must\" + 0.013*\"meaning\" + 0.013*\"service\"\n",
      "2024-08-02 13:17:08,055 : INFO : topic #1 (0.080): 0.034*\"directive\" + 0.023*\"article\" + 0.016*\"must\" + 0.016*\"service\" + 0.015*\"national\" + 0.013*\"interpreted\" + 0.012*\"state\" + 0.012*\"consumer\" + 0.011*\"council\" + 0.011*\"law\"\n",
      "2024-08-02 13:17:08,055 : INFO : topic #2 (0.056): 0.034*\"directive\" + 0.033*\"consumer\" + 0.020*\"contract\" + 0.018*\"council\" + 0.017*\"article\" + 0.015*\"term\" + 0.013*\"must\" + 0.010*\"interpreted\" + 0.010*\"eec\" + 0.009*\"national\"\n",
      "2024-08-02 13:17:08,056 : INFO : topic #3 (0.090): 0.026*\"directive\" + 0.025*\"consumer\" + 0.021*\"article\" + 0.021*\"term\" + 0.019*\"council\" + 0.017*\"unfair\" + 0.017*\"court\" + 0.017*\"must\" + 0.015*\"national\" + 0.014*\"ec\"\n",
      "2024-08-02 13:17:08,056 : INFO : topic #4 (0.027): 0.025*\"article\" + 0.022*\"mark\" + 0.017*\"directive\" + 0.015*\"trade\" + 0.015*\"regulation\" + 0.014*\"trade_mark\" + 0.012*\"council\" + 0.012*\"eec\" + 0.011*\"state\" + 0.011*\"product\"\n",
      "2024-08-02 13:17:08,056 : INFO : topic diff=0.244704, rho=0.500000\n",
      "2024-08-02 13:17:08,057 : INFO : PROGRESS: pass 3, at document #440/440\n",
      "2024-08-02 13:17:08,154 : INFO : optimized alpha [0.08138122, 0.07277976, 0.053813435, 0.08281483, 0.027565086]\n",
      "2024-08-02 13:17:08,155 : INFO : topic #0 (0.081): 0.030*\"directive\" + 0.028*\"article\" + 0.024*\"ec\" + 0.023*\"council\" + 0.022*\"european\" + 0.021*\"regulation\" + 0.018*\"parliament\" + 0.014*\"must\" + 0.013*\"meaning\" + 0.012*\"service\"\n",
      "2024-08-02 13:17:08,156 : INFO : topic #1 (0.073): 0.035*\"directive\" + 0.023*\"article\" + 0.017*\"service\" + 0.016*\"must\" + 0.015*\"national\" + 0.013*\"interpreted\" + 0.012*\"state\" + 0.011*\"council\" + 0.011*\"law\" + 0.011*\"consumer\"\n",
      "2024-08-02 13:17:08,156 : INFO : topic #2 (0.054): 0.034*\"consumer\" + 0.034*\"directive\" + 0.021*\"contract\" + 0.018*\"council\" + 0.017*\"article\" + 0.016*\"term\" + 0.013*\"must\" + 0.011*\"interpreted\" + 0.010*\"eec\" + 0.010*\"national\"\n",
      "2024-08-02 13:17:08,157 : INFO : topic #3 (0.083): 0.026*\"consumer\" + 0.026*\"directive\" + 0.022*\"term\" + 0.021*\"article\" + 0.019*\"unfair\" + 0.018*\"council\" + 0.018*\"court\" + 0.017*\"must\" + 0.016*\"national\" + 0.015*\"contract\"\n",
      "2024-08-02 13:17:08,157 : INFO : topic #4 (0.028): 0.026*\"article\" + 0.021*\"mark\" + 0.015*\"directive\" + 0.015*\"regulation\" + 0.015*\"trade\" + 0.014*\"trade_mark\" + 0.013*\"product\" + 0.012*\"council\" + 0.012*\"state\" + 0.012*\"eec\"\n",
      "2024-08-02 13:17:08,157 : INFO : topic diff=0.192904, rho=0.447214\n",
      "2024-08-02 13:17:08,159 : INFO : PROGRESS: pass 4, at document #440/440\n",
      "2024-08-02 13:17:08,249 : INFO : optimized alpha [0.07648511, 0.067922294, 0.05211918, 0.07843359, 0.02802926]\n",
      "2024-08-02 13:17:08,250 : INFO : topic #0 (0.076): 0.030*\"directive\" + 0.027*\"article\" + 0.026*\"ec\" + 0.024*\"council\" + 0.023*\"european\" + 0.021*\"regulation\" + 0.019*\"parliament\" + 0.014*\"must\" + 0.013*\"meaning\" + 0.012*\"service\"\n",
      "2024-08-02 13:17:08,251 : INFO : topic #1 (0.068): 0.035*\"directive\" + 0.023*\"article\" + 0.018*\"service\" + 0.016*\"must\" + 0.014*\"national\" + 0.013*\"interpreted\" + 0.012*\"state\" + 0.011*\"law\" + 0.011*\"council\" + 0.010*\"member\"\n",
      "2024-08-02 13:17:08,251 : INFO : topic #2 (0.052): 0.035*\"consumer\" + 0.034*\"directive\" + 0.021*\"contract\" + 0.017*\"council\" + 0.017*\"article\" + 0.016*\"term\" + 0.013*\"must\" + 0.011*\"interpreted\" + 0.011*\"eec\" + 0.010*\"national\"\n",
      "2024-08-02 13:17:08,251 : INFO : topic #3 (0.078): 0.028*\"consumer\" + 0.026*\"directive\" + 0.024*\"term\" + 0.021*\"article\" + 0.019*\"unfair\" + 0.018*\"court\" + 0.018*\"must\" + 0.017*\"council\" + 0.017*\"national\" + 0.016*\"contract\"\n",
      "2024-08-02 13:17:08,252 : INFO : topic #4 (0.028): 0.026*\"article\" + 0.020*\"mark\" + 0.016*\"regulation\" + 0.014*\"product\" + 0.014*\"directive\" + 0.014*\"trade\" + 0.013*\"state\" + 0.013*\"trade_mark\" + 0.013*\"eec\" + 0.012*\"council\"\n",
      "2024-08-02 13:17:08,252 : INFO : topic diff=0.158295, rho=0.408248\n",
      "2024-08-02 13:17:08,253 : INFO : PROGRESS: pass 5, at document #440/440\n",
      "2024-08-02 13:17:08,339 : INFO : optimized alpha [0.07292135, 0.06448655, 0.050778713, 0.07543163, 0.02882639]\n",
      "2024-08-02 13:17:08,340 : INFO : topic #0 (0.073): 0.031*\"directive\" + 0.027*\"article\" + 0.026*\"ec\" + 0.025*\"council\" + 0.023*\"european\" + 0.022*\"regulation\" + 0.020*\"parliament\" + 0.014*\"must\" + 0.013*\"meaning\" + 0.012*\"service\"\n",
      "2024-08-02 13:17:08,341 : INFO : topic #1 (0.064): 0.036*\"directive\" + 0.023*\"article\" + 0.019*\"service\" + 0.016*\"must\" + 0.014*\"national\" + 0.013*\"interpreted\" + 0.012*\"state\" + 0.011*\"law\" + 0.011*\"council\" + 0.010*\"member\"\n",
      "2024-08-02 13:17:08,341 : INFO : topic #2 (0.051): 0.035*\"consumer\" + 0.035*\"directive\" + 0.021*\"contract\" + 0.018*\"article\" + 0.017*\"council\" + 0.016*\"term\" + 0.013*\"must\" + 0.011*\"interpreted\" + 0.011*\"eec\" + 0.010*\"national\"\n",
      "2024-08-02 13:17:08,342 : INFO : topic #3 (0.075): 0.029*\"consumer\" + 0.026*\"directive\" + 0.025*\"term\" + 0.021*\"article\" + 0.020*\"unfair\" + 0.019*\"court\" + 0.018*\"must\" + 0.018*\"national\" + 0.017*\"council\" + 0.017*\"contract\"\n",
      "2024-08-02 13:17:08,342 : INFO : topic #4 (0.029): 0.026*\"article\" + 0.018*\"mark\" + 0.016*\"regulation\" + 0.016*\"product\" + 0.015*\"state\" + 0.014*\"directive\" + 0.013*\"member\" + 0.013*\"trade\" + 0.013*\"eec\" + 0.012*\"council\"\n",
      "2024-08-02 13:17:08,342 : INFO : topic diff=0.131546, rho=0.377964\n",
      "2024-08-02 13:17:08,344 : INFO : PROGRESS: pass 6, at document #440/440\n",
      "2024-08-02 13:17:08,428 : INFO : optimized alpha [0.07050963, 0.062047347, 0.049621407, 0.073356, 0.029750342]\n",
      "2024-08-02 13:17:08,429 : INFO : topic #0 (0.071): 0.031*\"directive\" + 0.027*\"article\" + 0.027*\"ec\" + 0.025*\"council\" + 0.024*\"european\" + 0.022*\"regulation\" + 0.020*\"parliament\" + 0.014*\"must\" + 0.013*\"meaning\" + 0.012*\"interpreted\"\n",
      "2024-08-02 13:17:08,429 : INFO : topic #1 (0.062): 0.036*\"directive\" + 0.023*\"article\" + 0.020*\"service\" + 0.016*\"must\" + 0.014*\"national\" + 0.013*\"interpreted\" + 0.012*\"state\" + 0.011*\"council\" + 0.011*\"law\" + 0.011*\"cost\"\n",
      "2024-08-02 13:17:08,430 : INFO : topic #2 (0.050): 0.035*\"consumer\" + 0.035*\"directive\" + 0.021*\"contract\" + 0.018*\"article\" + 0.017*\"council\" + 0.016*\"term\" + 0.013*\"must\" + 0.011*\"eec\" + 0.011*\"interpreted\" + 0.009*\"national\"\n",
      "2024-08-02 13:17:08,430 : INFO : topic #3 (0.073): 0.030*\"consumer\" + 0.027*\"term\" + 0.026*\"directive\" + 0.021*\"unfair\" + 0.020*\"article\" + 0.019*\"court\" + 0.019*\"must\" + 0.018*\"national\" + 0.018*\"contract\" + 0.016*\"council\"\n",
      "2024-08-02 13:17:08,430 : INFO : topic #4 (0.030): 0.026*\"article\" + 0.018*\"product\" + 0.017*\"regulation\" + 0.017*\"mark\" + 0.015*\"state\" + 0.014*\"member\" + 0.013*\"eec\" + 0.013*\"directive\" + 0.012*\"council\" + 0.012*\"trade\"\n",
      "2024-08-02 13:17:08,431 : INFO : topic diff=0.111076, rho=0.353553\n",
      "2024-08-02 13:17:08,432 : INFO : PROGRESS: pass 7, at document #440/440\n",
      "2024-08-02 13:17:08,515 : INFO : optimized alpha [0.06874838, 0.060276918, 0.04865682, 0.071783766, 0.030747399]\n",
      "2024-08-02 13:17:08,516 : INFO : topic #0 (0.069): 0.032*\"directive\" + 0.028*\"ec\" + 0.027*\"article\" + 0.026*\"council\" + 0.025*\"european\" + 0.022*\"regulation\" + 0.021*\"parliament\" + 0.014*\"must\" + 0.013*\"meaning\" + 0.012*\"interpreted\"\n",
      "2024-08-02 13:17:08,516 : INFO : topic #1 (0.060): 0.037*\"directive\" + 0.023*\"article\" + 0.020*\"service\" + 0.016*\"must\" + 0.014*\"national\" + 0.013*\"interpreted\" + 0.012*\"state\" + 0.011*\"cost\" + 0.011*\"council\" + 0.011*\"law\"\n",
      "2024-08-02 13:17:08,517 : INFO : topic #2 (0.049): 0.036*\"consumer\" + 0.035*\"directive\" + 0.021*\"contract\" + 0.018*\"article\" + 0.017*\"council\" + 0.015*\"term\" + 0.013*\"must\" + 0.011*\"eec\" + 0.011*\"interpreted\" + 0.009*\"provision\"\n",
      "2024-08-02 13:17:08,517 : INFO : topic #3 (0.072): 0.031*\"consumer\" + 0.028*\"term\" + 0.026*\"directive\" + 0.021*\"unfair\" + 0.020*\"article\" + 0.019*\"court\" + 0.019*\"must\" + 0.019*\"national\" + 0.018*\"contract\" + 0.016*\"council\"\n",
      "2024-08-02 13:17:08,517 : INFO : topic #4 (0.031): 0.026*\"article\" + 0.020*\"product\" + 0.018*\"regulation\" + 0.016*\"state\" + 0.016*\"mark\" + 0.015*\"member\" + 0.013*\"eec\" + 0.012*\"council\" + 0.012*\"directive\" + 0.012*\"trade\"\n",
      "2024-08-02 13:17:08,517 : INFO : topic diff=0.094551, rho=0.333333\n",
      "2024-08-02 13:17:08,519 : INFO : PROGRESS: pass 8, at document #440/440\n",
      "2024-08-02 13:17:08,599 : INFO : optimized alpha [0.06746113, 0.05889149, 0.048005052, 0.070596, 0.031774268]\n",
      "2024-08-02 13:17:08,600 : INFO : topic #0 (0.067): 0.032*\"directive\" + 0.029*\"ec\" + 0.027*\"article\" + 0.026*\"council\" + 0.025*\"european\" + 0.022*\"regulation\" + 0.022*\"parliament\" + 0.014*\"must\" + 0.013*\"meaning\" + 0.012*\"interpreted\"\n",
      "2024-08-02 13:17:08,601 : INFO : topic #1 (0.059): 0.037*\"directive\" + 0.023*\"article\" + 0.021*\"service\" + 0.016*\"must\" + 0.014*\"national\" + 0.013*\"interpreted\" + 0.012*\"state\" + 0.011*\"cost\" + 0.011*\"council\" + 0.011*\"law\"\n",
      "2024-08-02 13:17:08,601 : INFO : topic #2 (0.048): 0.036*\"consumer\" + 0.035*\"directive\" + 0.021*\"contract\" + 0.018*\"article\" + 0.017*\"council\" + 0.015*\"term\" + 0.013*\"must\" + 0.011*\"eec\" + 0.011*\"interpreted\" + 0.010*\"agreement\"\n",
      "2024-08-02 13:17:08,601 : INFO : topic #3 (0.071): 0.032*\"consumer\" + 0.029*\"term\" + 0.026*\"directive\" + 0.022*\"unfair\" + 0.020*\"article\" + 0.020*\"court\" + 0.019*\"must\" + 0.019*\"contract\" + 0.019*\"national\" + 0.016*\"council\"\n",
      "2024-08-02 13:17:08,602 : INFO : topic #4 (0.032): 0.026*\"article\" + 0.021*\"product\" + 0.018*\"regulation\" + 0.017*\"state\" + 0.015*\"mark\" + 0.015*\"member\" + 0.013*\"eec\" + 0.012*\"council\" + 0.012*\"directive\" + 0.011*\"trade\"\n",
      "2024-08-02 13:17:08,602 : INFO : topic diff=0.081631, rho=0.316228\n",
      "2024-08-02 13:17:08,604 : INFO : PROGRESS: pass 9, at document #440/440\n",
      "2024-08-02 13:17:08,681 : INFO : optimized alpha [0.06652631, 0.057848774, 0.047382586, 0.0698119, 0.03285942]\n",
      "2024-08-02 13:17:08,682 : INFO : topic #0 (0.067): 0.032*\"directive\" + 0.030*\"ec\" + 0.027*\"article\" + 0.027*\"council\" + 0.026*\"european\" + 0.022*\"regulation\" + 0.022*\"parliament\" + 0.014*\"must\" + 0.013*\"meaning\" + 0.012*\"interpreted\"\n",
      "2024-08-02 13:17:08,682 : INFO : topic #1 (0.058): 0.037*\"directive\" + 0.023*\"article\" + 0.022*\"service\" + 0.016*\"must\" + 0.014*\"national\" + 0.013*\"interpreted\" + 0.012*\"state\" + 0.012*\"cost\" + 0.011*\"council\" + 0.010*\"law\"\n",
      "2024-08-02 13:17:08,683 : INFO : topic #2 (0.047): 0.036*\"consumer\" + 0.036*\"directive\" + 0.020*\"contract\" + 0.018*\"article\" + 0.017*\"council\" + 0.015*\"term\" + 0.013*\"must\" + 0.011*\"eec\" + 0.011*\"interpreted\" + 0.010*\"agreement\"\n",
      "2024-08-02 13:17:08,683 : INFO : topic #3 (0.070): 0.033*\"consumer\" + 0.029*\"term\" + 0.026*\"directive\" + 0.022*\"unfair\" + 0.020*\"article\" + 0.020*\"court\" + 0.020*\"must\" + 0.020*\"contract\" + 0.019*\"national\" + 0.015*\"interpreted\"\n",
      "2024-08-02 13:17:08,684 : INFO : topic #4 (0.033): 0.026*\"article\" + 0.022*\"product\" + 0.018*\"regulation\" + 0.017*\"state\" + 0.016*\"member\" + 0.015*\"mark\" + 0.013*\"eec\" + 0.012*\"council\" + 0.012*\"directive\" + 0.011*\"trade\"\n",
      "2024-08-02 13:17:08,684 : INFO : topic diff=0.070819, rho=0.301511\n",
      "2024-08-02 13:17:08,685 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=2035, num_topics=5, decay=0.5, chunksize=2000> in 1.23s', 'datetime': '2024-08-02T13:17:08.685872', 'gensim': '4.3.2', 'python': '3.11.9 (main, Jul 16 2024, 11:07:54) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'created'}\n",
      "2024-08-02 13:17:08,686 : INFO : LdaState lifecycle event {'fname_or_handle': 'lda_models/topics_5_passes_10_iterations_200/trained_model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-08-02T13:17:08.686468', 'gensim': '4.3.2', 'python': '3.11.9 (main, Jul 16 2024, 11:07:54) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'saving'}\n",
      "2024-08-02 13:17:08,687 : INFO : saved lda_models/topics_5_passes_10_iterations_200/trained_model.state\n",
      "2024-08-02 13:17:08,688 : INFO : LdaModel lifecycle event {'fname_or_handle': 'lda_models/topics_5_passes_10_iterations_200/trained_model', 'separately': \"['expElogbeta', 'sstats']\", 'sep_limit': 10485760, 'ignore': ['dispatcher', 'state', 'id2word'], 'datetime': '2024-08-02T13:17:08.688508', 'gensim': '4.3.2', 'python': '3.11.9 (main, Jul 16 2024, 11:07:54) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'saving'}\n",
      "2024-08-02 13:17:08,688 : INFO : storing np array 'expElogbeta' to lda_models/topics_5_passes_10_iterations_200/trained_model.expElogbeta.npy\n",
      "2024-08-02 13:17:08,689 : INFO : not storing attribute dispatcher\n",
      "2024-08-02 13:17:08,689 : INFO : not storing attribute state\n",
      "2024-08-02 13:17:08,690 : INFO : not storing attribute id2word\n",
      "2024-08-02 13:17:08,690 : INFO : saved lda_models/topics_5_passes_10_iterations_200/trained_model\n",
      "2024-08-02 13:17:08,691 : INFO : Dictionary lifecycle event {'fname_or_handle': 'lda_models/topics_5_passes_10_iterations_200/dictionary', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-08-02T13:17:08.691142', 'gensim': '4.3.2', 'python': '3.11.9 (main, Jul 16 2024, 11:07:54) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'saving'}\n",
      "2024-08-02 13:17:08,692 : INFO : saved lda_models/topics_5_passes_10_iterations_200/dictionary\n",
      "2024-08-02 13:17:08,707 : INFO : topic #0 (0.067): 0.032*\"directive\" + 0.030*\"ec\" + 0.027*\"article\" + 0.027*\"council\" + 0.026*\"european\" + 0.022*\"regulation\" + 0.022*\"parliament\" + 0.014*\"must\" + 0.013*\"meaning\" + 0.012*\"interpreted\"\n",
      "2024-08-02 13:17:08,709 : INFO : topic #1 (0.058): 0.037*\"directive\" + 0.023*\"article\" + 0.022*\"service\" + 0.016*\"must\" + 0.014*\"national\" + 0.013*\"interpreted\" + 0.012*\"state\" + 0.012*\"cost\" + 0.011*\"council\" + 0.010*\"law\"\n",
      "2024-08-02 13:17:08,710 : INFO : topic #2 (0.047): 0.036*\"consumer\" + 0.036*\"directive\" + 0.020*\"contract\" + 0.018*\"article\" + 0.017*\"council\" + 0.015*\"term\" + 0.013*\"must\" + 0.011*\"eec\" + 0.011*\"interpreted\" + 0.010*\"agreement\"\n",
      "2024-08-02 13:17:08,712 : INFO : topic #3 (0.070): 0.033*\"consumer\" + 0.029*\"term\" + 0.026*\"directive\" + 0.022*\"unfair\" + 0.020*\"article\" + 0.020*\"court\" + 0.020*\"must\" + 0.020*\"contract\" + 0.019*\"national\" + 0.015*\"interpreted\"\n",
      "2024-08-02 13:17:08,714 : INFO : topic #4 (0.033): 0.026*\"article\" + 0.022*\"product\" + 0.018*\"regulation\" + 0.017*\"state\" + 0.016*\"member\" + 0.015*\"mark\" + 0.013*\"eec\" + 0.012*\"council\" + 0.012*\"directive\" + 0.011*\"trade\"\n",
      "2024-08-02 13:17:08,836 : INFO : -6.273 per-word bound, 77.3 perplexity estimate based on a held-out corpus of 440 documents with 44579 words\n",
      "2024-08-02 13:17:08,837 : INFO : using ParallelWordOccurrenceAccumulator<processes=9, batch_size=64> to estimate probabilities from sliding windows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model trained successfully\n",
      "Model saved to lda_models/topics_5_passes_10_iterations_200/trained_model\n",
      "Dictionary saved to lda_models/topics_5_passes_10_iterations_200/dictionary\n",
      "Corpus saved to lda_models/topics_5_passes_10_iterations_200/corpus\n",
      "Number of unique tokens: 2035\n",
      "Number of documents: 440\n",
      "Average topic coherence: -1.0921.\n",
      "\n",
      "Top topics:\n",
      "[(0,\n",
      "  '0.032*\"directive\" + 0.030*\"ec\" + 0.027*\"article\" + 0.027*\"council\" + '\n",
      "  '0.026*\"european\" + 0.022*\"regulation\" + 0.022*\"parliament\" + 0.014*\"must\" + '\n",
      "  '0.013*\"meaning\" + 0.012*\"interpreted\"'),\n",
      " (1,\n",
      "  '0.037*\"directive\" + 0.023*\"article\" + 0.022*\"service\" + 0.016*\"must\" + '\n",
      "  '0.014*\"national\" + 0.013*\"interpreted\" + 0.012*\"state\" + 0.012*\"cost\" + '\n",
      "  '0.011*\"council\" + 0.010*\"law\"'),\n",
      " (2,\n",
      "  '0.036*\"consumer\" + 0.036*\"directive\" + 0.020*\"contract\" + 0.018*\"article\" + '\n",
      "  '0.017*\"council\" + 0.015*\"term\" + 0.013*\"must\" + 0.011*\"eec\" + '\n",
      "  '0.011*\"interpreted\" + 0.010*\"agreement\"'),\n",
      " (3,\n",
      "  '0.033*\"consumer\" + 0.029*\"term\" + 0.026*\"directive\" + 0.022*\"unfair\" + '\n",
      "  '0.020*\"article\" + 0.020*\"court\" + 0.020*\"must\" + 0.020*\"contract\" + '\n",
      "  '0.019*\"national\" + 0.015*\"interpreted\"'),\n",
      " (4,\n",
      "  '0.026*\"article\" + 0.022*\"product\" + 0.018*\"regulation\" + 0.017*\"state\" + '\n",
      "  '0.016*\"member\" + 0.015*\"mark\" + 0.013*\"eec\" + 0.012*\"council\" + '\n",
      "  '0.012*\"directive\" + 0.011*\"trade\"')]\n",
      "\n",
      "Perplexity:  -6.273072865441498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 13:17:09,781 : INFO : 2 batches submitted to accumulate stats from 128 documents (347 virtual)\n",
      "2024-08-02 13:17:09,784 : INFO : 3 batches submitted to accumulate stats from 192 documents (1274 virtual)\n",
      "2024-08-02 13:17:10,834 : INFO : 9 accumulators retrieved from output queue\n",
      "2024-08-02 13:17:10,844 : INFO : accumulated word occurrence stats for 11447 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4517129413820314\n",
      "\n",
      "Document 0 topics:\n",
      "[(1, 0.9984667)]\n",
      "Best topic: 1\n",
      "Top words in this topic:\n",
      "[('directive', 0.037215818),\n",
      " ('article', 0.023045152),\n",
      " ('service', 0.021931587),\n",
      " ('must', 0.01592759),\n",
      " ('national', 0.013697454),\n",
      " ('interpreted', 0.012828682),\n",
      " ('state', 0.011819984),\n",
      " ('cost', 0.011662337),\n",
      " ('council', 0.010960338),\n",
      " ('law', 0.010464187)]\n",
      "Original document: pricing interconnection related provision number portability referred article directive ec european parliament council march universal service user right relating electronic communication network service universal service directive concern traffic cost number...\n",
      "\n",
      "\n",
      "Document 1 topics:\n",
      "[(0, 0.694997), (2, 0.30318424)]\n",
      "Best topic: 0\n",
      "Top words in this topic:\n",
      "[('directive', 0.032075357),\n",
      " ('ec', 0.029609142),\n",
      " ('article', 0.027314719),\n",
      " ('council', 0.026978357),\n",
      " ('european', 0.025506597),\n",
      " ('regulation', 0.0223595),\n",
      " ('parliament', 0.022057863),\n",
      " ('must', 0.014074753),\n",
      " ('meaning', 0.013009776),\n",
      " ('interpreted', 0.011972018)]\n",
      "Original document: ground court fifth chamber hereby rule article council directive eec april unfair term consumer contract must interpreted meaning natural person becomes member scheme implemented commercial company allowing inter alia certain...\n",
      "\n",
      "\n",
      "Document 2 topics:\n",
      "[(4, 0.998337)]\n",
      "Best topic: 4\n",
      "Top words in this topic:\n",
      "[('article', 0.025950326),\n",
      " ('product', 0.022364106),\n",
      " ('regulation', 0.018328639),\n",
      " ('state', 0.017266598),\n",
      " ('member', 0.015565686),\n",
      " ('mark', 0.0147198085),\n",
      " ('eec', 0.013280731),\n",
      " ('council', 0.012461004),\n",
      " ('directive', 0.011773821),\n",
      " ('trade', 0.010905006)]\n",
      "Original document: article first council directive eec december approximate law member state relating trade mark article 3a council directive eec september concerning misleading comparative advertising amended directive ec european parliament council october...\n",
      "\n",
      "\n",
      "Document 3 topics:\n",
      "[(1, 0.62570804), (2, 0.37347946)]\n",
      "Best topic: 1\n",
      "Top words in this topic:\n",
      "[('directive', 0.037215818),\n",
      " ('article', 0.023045152),\n",
      " ('service', 0.021931587),\n",
      " ('must', 0.01592759),\n",
      " ('national', 0.013697454),\n",
      " ('interpreted', 0.012828682),\n",
      " ('state', 0.011819984),\n",
      " ('cost', 0.011662337),\n",
      " ('council', 0.010960338),\n",
      " ('law', 0.010464187)]\n",
      "Original document: article article directive ec european parliament council april credit agreement consumer repealing council directive eec must interpreted precluding national provision consumer credit lay method calculating maximum non interest credit cost...\n",
      "\n",
      "\n",
      "Document 4 topics:\n",
      "[(1, 0.068152025), (2, 0.32938725), (3, 0.6012376)]\n",
      "Best topic: 3\n",
      "Top words in this topic:\n",
      "[('consumer', 0.032742865),\n",
      " ('term', 0.029360019),\n",
      " ('directive', 0.026100785),\n",
      " ('unfair', 0.021821132),\n",
      " ('article', 0.02036113),\n",
      " ('court', 0.020062897),\n",
      " ('must', 0.019755566),\n",
      " ('contract', 0.019752646),\n",
      " ('national', 0.01917486),\n",
      " ('interpreted', 0.015430455)]\n",
      "Original document: council directive eec april unfair term consumer contract must interpreted precluding national legislation issue main proceeding allows recovery debt based potentially unfair contractual term extrajudicial enforcement charge immovable property provided...\n",
      "\n",
      "\n",
      "Running model with parameters: num_topics=10, chunksize=2000, passes=10, iterations=400\n",
      "Loaded 880 documents\n",
      "Sample document: ['pricing', 'interconnection', 'related', 'provision', 'number', 'portability', 'referred', 'article', 'directive', 'ec', 'european', 'parliament', 'council', 'march', 'universal', 'service', 'user', 'right', 'relating', 'electronic', 'communication', 'network', 'service', 'universal', 'service', 'directive', 'concern', 'traffic', 'cost', 'number', 'ported', 'set', 'cost', 'incurred', 'mobile', 'telephone', 'operator', 'implement', 'request', 'number', 'porting', 'article', 'directive', 'preclude', 'adoption', 'national', 'measure', 'laying', 'specific', 'method', 'used', 'calculating', 'cost', 'fix', 'advance', 'basis', 'abstract', 'model', 'cost', 'maximum', 'price', 'may', 'charged', 'donor', 'operator', 'recipient', 'operator', 'set', 'cost', 'provided', 'price', 'fixed', 'basis', 'cost', 'way', 'consumer', 'dissuaded', 'making', 'use', 'facility', 'portability', 'article', 'directive', 'ec', 'european', 'parliament', 'council', 'march', 'common', 'regulatory', 'framework', 'electronic', 'communication', 'network', 'service', 'framework', 'directive', 'must', 'interpreted', 'meaning']...\n",
      "An error occurred: expected string or bytes-like object, got 'list'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 178\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mfor\u001b[39;00m num_topics, chunksize, passes, iterations \u001b[39min\u001b[39;00m param_combinations:\n\u001b[1;32m    177\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mRunning model with parameters: num_topics=\u001b[39m\u001b[39m{\u001b[39;00mnum_topics\u001b[39m}\u001b[39;00m\u001b[39m, chunksize=\u001b[39m\u001b[39m{\u001b[39;00mchunksize\u001b[39m}\u001b[39;00m\u001b[39m, passes=\u001b[39m\u001b[39m{\u001b[39;00mpasses\u001b[39m}\u001b[39;00m\u001b[39m, iterations=\u001b[39m\u001b[39m{\u001b[39;00miterations\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m     modeler\u001b[39m.\u001b[39;49mrun_with_params(num_topics, chunksize, passes, iterations)\n",
      "Cell \u001b[0;32mIn[13], line 149\u001b[0m, in \u001b[0;36mTopicModeler.run_with_params\u001b[0;34m(self, num_topics, chunksize, passes, iterations)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_documents()\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess_documents()\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_bigrams()\n\u001b[1;32m    151\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_dictionary_and_corpus()\n",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m, in \u001b[0;36mTopicModeler.preprocess_documents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_documents\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocs \u001b[39m=\u001b[39m [[token\u001b[39m.\u001b[39;49mlower() \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mtokenize(doc)] \u001b[39mfor\u001b[39;49;00m doc \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocs]\n\u001b[1;32m     37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocs \u001b[39m=\u001b[39m [[token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39misnumeric() \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(token) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocs]\n\u001b[1;32m     38\u001b[0m     stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_documents\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocs \u001b[39m=\u001b[39m [[token\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mtokenize(doc)] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocs]\n\u001b[1;32m     37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocs \u001b[39m=\u001b[39m [[token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39misnumeric() \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(token) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocs]\n\u001b[1;32m     38\u001b[0m     stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/tokenize/regexp.py:133\u001b[0m, in \u001b[0;36mRegexpTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_regexp\u001b[39m.\u001b[39msplit(text)\n\u001b[1;32m    131\u001b[0m \u001b[39m# If our regexp matches tokens, use re.findall:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_regexp\u001b[39m.\u001b[39mfindall(text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases, LdaModel\n",
    "from gensim.models import phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "class TopicModeler:\n",
    "    def __init__(self, input_path):\n",
    "        self.input_path = input_path\n",
    "        self.docs = []\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        self.model = None\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    def load_documents(self):\n",
    "        for filename in os.listdir(self.input_path):\n",
    "            file_path = os.path.join(self.input_path, filename)\n",
    "            if os.path.isfile(file_path) and filename.endswith('.txt'):  # Ensure it's a text file\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read().strip()\n",
    "                    if isinstance(content, str):\n",
    "                        self.docs.append(content)\n",
    "        print(f\"Loaded {len(self.docs)} documents\")\n",
    "        if self.docs:\n",
    "            print(f\"Sample document: {self.docs[0][:100]}...\")\n",
    "\n",
    "    def preprocess_documents(self):\n",
    "        self.docs = [[token.lower() for token in self.tokenizer.tokenize(doc)] for doc in self.docs if isinstance(doc, str)]\n",
    "        self.docs = [[token for token in doc if not token.isnumeric() and len(token) > 1] for doc in self.docs]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        self.docs = [[word for word in doc if word not in stop_words] for doc in self.docs]\n",
    "        self.docs = [[self.lemmatizer.lemmatize(token) for token in doc] for doc in self.docs]\n",
    "        print(f\"After preprocessing: {len(self.docs)} documents\")\n",
    "        if self.docs:\n",
    "            print(f\"Sample preprocessed document: {self.docs[0][:10]}...\")\n",
    "\n",
    "    def add_bigrams(self):\n",
    "        bigram = Phrases(self.docs, min_count=5, threshold=100)\n",
    "        for idx in range(len(self.docs)):\n",
    "            for token in bigram[self.docs[idx]]:\n",
    "                if '_' in token:\n",
    "                    self.docs[idx].append(token)\n",
    "        print(f\"Added bigrams. Sample document: {self.docs[0][:15]}...\")\n",
    "\n",
    "    def create_dictionary_and_corpus(self):\n",
    "        self.dictionary = Dictionary(self.docs)\n",
    "        original_size = len(self.dictionary)\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=0.9)\n",
    "        self.corpus = [self.dictionary.doc2bow(doc) for doc in self.docs]\n",
    "        print(f\"Dictionary size: {len(self.dictionary)} (reduced from {original_size})\")\n",
    "        print(f\"Corpus size: {len(self.corpus)}\")\n",
    "        if self.corpus:\n",
    "            print(f\"Sample corpus entry: {self.corpus[0][:10]}\")\n",
    "\n",
    "    def train_lda_model(self, num_topics=6, chunksize=2000, passes=20, iterations=400, alpha='auto', eta='auto'):\n",
    "        if not self.corpus or not self.dictionary:\n",
    "            raise ValueError(\"Corpus or dictionary is empty. Check your preprocessing steps.\")\n",
    "        \n",
    "        print(f\"Training LDA model with num_topics={num_topics}, chunksize={chunksize}, passes={passes}, iterations={iterations}, alpha={alpha}, eta={eta}\")\n",
    "        \n",
    "        self.model = LdaModel(\n",
    "            corpus=self.corpus,\n",
    "            id2word=self.dictionary,\n",
    "            chunksize=chunksize,\n",
    "            alpha=alpha,\n",
    "            eta=eta,\n",
    "            iterations=iterations,\n",
    "            num_topics=num_topics,\n",
    "            passes=passes,\n",
    "            eval_every=None,\n",
    "            random_state=42\n",
    "        )\n",
    "        print(\"LDA model trained successfully\")\n",
    "\n",
    "    def print_model_info(self):\n",
    "        print('Number of unique tokens: %d' % len(self.dictionary))\n",
    "        print('Number of documents: %d' % len(self.corpus))\n",
    "        top_topics = self.model.top_topics(self.corpus)\n",
    "        avg_topic_coherence = sum([t[1] for t in top_topics]) / self.model.num_topics\n",
    "        print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "        print(\"\\nTop topics:\")\n",
    "        pprint(self.model.print_topics())\n",
    "        print('\\nPerplexity: ', self.model.log_perplexity(self.corpus))\n",
    "        coherence_model_lda = CoherenceModel(model=self.model, texts=self.docs, dictionary=self.dictionary, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "    def visualize_topics(self):\n",
    "        vis = pyLDAvis.gensim.prepare(self.model, self.corpus, self.dictionary)\n",
    "        return vis\n",
    "\n",
    "    def analyze_documents(self):\n",
    "        for i, doc in enumerate(self.docs[:5]):\n",
    "            bow = self.dictionary.doc2bow(doc)\n",
    "            doc_topics = self.model.get_document_topics(bow)\n",
    "            print(f\"\\nDocument {i} topics:\")\n",
    "            pprint(doc_topics)\n",
    "            best_topic = max(doc_topics, key=lambda x: x[1])\n",
    "            print(f\"Best topic: {best_topic[0]}\")\n",
    "            print(f\"Top words in this topic:\")\n",
    "            pprint(self.model.show_topic(best_topic[0]))\n",
    "            print(f\"Original document: {' '.join(doc[:30])}...\")\n",
    "            print()\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model has been trained yet.\")\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        self.model.save(file_path)\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"No model file found at {file_path}\")\n",
    "        self.model = LdaModel.load(file_path)\n",
    "        print(f\"Model loaded from {file_path}\")\n",
    "\n",
    "    def save_dictionary_and_corpus(self, dict_path, corpus_path):\n",
    "        if self.dictionary is None or self.corpus is None:\n",
    "            raise ValueError(\"Dictionary and corpus have not been created yet.\")\n",
    "        os.makedirs(os.path.dirname(dict_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(corpus_path), exist_ok=True)\n",
    "        self.dictionary.save(dict_path)\n",
    "        with open(corpus_path, 'wb') as f:\n",
    "            pickle.dump(self.corpus, f)\n",
    "        print(f\"Dictionary saved to {dict_path}\")\n",
    "        print(f\"Corpus saved to {corpus_path}\")\n",
    "\n",
    "    def load_dictionary_and_corpus(self, dict_path, corpus_path):\n",
    "        if not os.path.exists(dict_path) or not os.path.exists(corpus_path):\n",
    "            raise FileNotFoundError(f\"Dictionary or corpus file not found.\")\n",
    "        self.dictionary = Dictionary.load(dict_path)\n",
    "        with open(corpus_path, 'rb') as f:\n",
    "            self.corpus = pickle.load(f)\n",
    "        print(f\"Dictionary loaded from {dict_path}\")\n",
    "        print(f\"Corpus loaded from {corpus_path}\")\n",
    "\n",
    "    def run_with_params(self, num_topics, chunksize, passes, iterations):\n",
    "        try:\n",
    "            self.load_documents()\n",
    "            self.preprocess_documents()\n",
    "            self.add_bigrams()\n",
    "            self.create_dictionary_and_corpus()\n",
    "            self.train_lda_model(num_topics=num_topics, chunksize=chunksize, passes=passes, iterations=iterations)\n",
    "            model_dir = f\"lda_models/topics_{num_topics}_passes_{passes}_iterations_{iterations}\"\n",
    "            self.save_model(os.path.join(model_dir, 'trained_model'))\n",
    "            self.save_dictionary_and_corpus(os.path.join(model_dir, 'dictionary'), os.path.join(model_dir, 'corpus'))\n",
    "            self.print_model_info()\n",
    "            vis = self.visualize_topics()\n",
    "            pyLDAvis.save_html(vis, os.path.join(model_dir, 'lda_visualization.html'))\n",
    "            self.analyze_documents()\n",
    "            return vis\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    modeler = TopicModeler('txt')\n",
    "\n",
    "    # Train and save models with different parameters\n",
    "    param_combinations = [\n",
    "        (5, 2000, 10, 200),\n",
    "        (10, 2000, 10, 400),\n",
    "        (15, 2000, 20, 200),\n",
    "        (10, 2000, 20, 400)\n",
    "    ]\n",
    "\n",
    "    for num_topics, chunksize, passes, iterations in param_combinations:\n",
    "        print(f\"\\nRunning model with parameters: num_topics={num_topics}, chunksize={chunksize}, passes={passes}, iterations={iterations}\")\n",
    "        modeler.run_with_params(num_topics, chunksize, passes, iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
